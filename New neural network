library(keras)
library(tfdatasets)

#################################################### Importing and normalizing the data
# model <- 

data <- read.csv("~/Desktop/Forskning och jobb/Prognostic calulation/Uppsala.data.csv")
set.seed(0)
sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7,0.3))
train.data  <- data[sample, ]
test.data   <- data[!sample, ]
table(train.data$eGOS)

train.data$eGOS <- train.data$eGOS - 1                                 ##### <-------- Python begins calculations from 0. So eGOS should be from 0 - 7.

train.data$motor <- train.data$motor - 1
train.data$pupil <- train.data$pupil - 1
train.data$CT <- train.data$CT - 1
train.data$age <- scale(train.data$age)


test.data$eGOS <- test.data$eGOS - 1                                 ##### <-------- Python begins calculations from 0. So eGOS should be from 0 - 7.

test.data$motor <- test.data$motor - 1
test.data$pupil <- test.data$pupil - 1
test.data$CT <- test.data$CT - 1
test.data$age <- scale(test.data$age)


###################################### Formatting the training data
train.features <- train.data[, c("motor", "pupil", "age", "CT")]
train.features$pupil <- to_categorical(train.features$pupil, 3)
train.features$motor <- to_categorical(train.features$motor, 6)
train.features$CT <- to_categorical(train.features$CT, 6)
train.x <- as.matrix(train.features)

train.y <- to_categorical(train.data$eGOS, 8) # <--- antalet labels


###################################### Formatting the testing data
test.features <- test.data[, c("motor", "pupil", "age", "CT")]
test.features$pupil <- to_categorical(test.features$pupil, 3)
test.features$motor <- to_categorical(test.features$motor, 6)
test.features$CT <- to_categorical(test.features$CT, 6)
test.x <- as.matrix(test.features)

test.y <- to_categorical(test.data$eGOS, 8) # <--- antalet labels

#### x = features och y = labels

################################# Designing the network


model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 10000, activation = 'relu', input_shape = c(16)) %>%  # <------- Input shape ?r antalet kolumner i train.x (antal features).
  layer_dense(units = 5000, activation = 'relu') %>%
  layer_dense(units = 1000, activation = 'relu') %>%
  layer_dense(units = 500, activation = 'relu') %>%
  layer_dense(units = 100, activation = 'relu') %>%
  layer_dense(units = 50, activation = 'relu') %>%
  layer_dense(units = 25, activation = 'relu') %>%
  layer_dense(units = 8, activation = 'softmax')

summary(model)

model %>% compile(loss = 'categorical_crossentropy', optimizer = "adam", metrics = "accuracy")

################################# Training the network
## kernel_regularizer = regularizer_l1_l2(l1 = 0.01, l2 = 0.01)



history <- model %>% fit(
  train.x, train.y, 
  epochs = 30, batch_size = 30, 
  validation_split = 0.2
)
plot(history)

######################## Evaluation of the network
evaluate(model, test.x, test.y)


######################################### MSE / MAE
probabilities <- predict(model, test.x)
predicted.egos <- apply(probabilities, 1, which.max)
table(predicted.egos)
table(test.data$eGOS+1)

actual.egos <- as.numeric(test.data$eGOS)

MSE <- mean((actual.egos - predicted.egos)^2)
MAE <- mean(abs(actual.egos - predicted.egos))

correct.prediction1 <- predicted.egos == actual.egos
accuracy <- sum(correct.prediction1) / length(correct.prediction1)

correct.prediction2 <- abs(predicted.egos - actual.egos) <= 1
AW1 <- sum(correct.prediction2) / length(correct.prediction2)

correct.prediction3 <- abs(predicted.egos - actual.egos) <= 2
AW2 <- sum(correct.prediction3) / length(correct.prediction3)

accuracy
AW1
AW2
MSE
MAE


############# saving model

save_model_hdf5(model, file="NN.h5")

library(glmnet)
library(ggplot2)

data <- read.csv("Data/Uppsaladata.CSV")

set.seed(42)

################################### Sample data

ind <- sample(2, nrow(data), replace=TRUE, prob = c(0.8, 0.2))
train.data <- data[ind==1,]
test.data <- data[ind==2,]

train.features <- train.data[, c("motor", "pupil", "age", "CT")]
x.train <- as.matrix(train.features)

test.features <- test.data[, c("motor", "pupil", "age", "CT")]
x.test <- as.matrix(test.features)

train.lable <- train.data[, c("eGOS")]
y.train <- as.matrix(train.lable)

test.lable <- test.data[, c("eGOS")]
y.test <- as.matrix(test.lable)

######################## Ridge regression (alpha = 0)

alpha0.fit <- cv.glmnet(x.train, y.train, type.measure="mse",
                        alpha=0, family="gaussian")

alpha0.predicted <-
  predict(alpha0.fit, s=alpha0.fit$lambda.1se, newx=x.test) #lambda.1se = optimal value for lambda in alpha0.fit

MSE.0 <- mean((y.test - alpha0.predicted)^2) #MSE for ridge regression
MAE.0 <- mean(abs(y.test - alpha0.predicted))


######################## Lasso regression (alpha = 1)
alpha1.fit <- cv.glmnet(x.train, y.train, type.measure="mse",
                        alpha=1, family="multinomial")

alpha1.predicted <-
  predict(alpha1.fit, s=alpha1.fit$lambda.1se, newx=x.test)

MSE.1 <- mean((y.test - alpha1.predicted)^2) #MSE for lasso regression
MAE.1 <- mean(abs(y.test - alpha1.predicted))

# Compare MSE to see which is best


####################### Elastic net regression (alpha = 0.5)

alpha0.5.fit <- cv.glmnet(x.train, y.train, type.measure="mse",
                          alpha=0.5, family="gaussian")

alpha0.5.predicted <-
  predict(alpha0.5.fit, s=alpha0.5.fit$lambda.1se, newx=x.test)

MSE.0.5 <-mean((y.test - alpha0.5.predicted)^2)
MAE.0.5 <- mean(abs(y.test - alpha0.5.predicted))

# Compare MSE of each three

###################### Finding optimal alpha value from 0.0 to 1.0 (combination of lasso and ridge regression)

list.of.fits <- list()
for (i in 0:10) {
  fit.name <- paste0("alpha", i/10) #creates names of alpha 0/10=0 to 1/10=1
  
  list.of.fits[[fit.name]] <-
    cv.glmnet(x.train, y.train, type.measure = "mse", alpha=i/10,
              family="gaussian")
}

results <- data.frame()
for (i in 0:10) {
  fit.name <- paste0("alpha", i/10)
  
  predicted <-
    predict(list.of.fits[[fit.name]],
            s=list.of.fits[[fit.name]]$lambda.1se, newx=x.test)
  
  mse <- mean((y.test - predicted)^2)
  
  temp <- data.frame(alpha=i/10, mse=mse, fit.name=fit.name)
  results <- rbind(results, temp)
}

results


eGOS <- round(predict(alpha0.fit, x.test))
head(eGOS)

############################## Optimal: alpha = 0
alpha0.fit <- cv.glmnet(x.train, y.train, type.measure="mse",
                          alpha=0, family="gaussian")

alpha0.predicted <-
  predict(alpha0.fit, s=alpha0.fit$lambda.1se, newx=x.test)
alpha0.predicted


#############MSE

MSE0 <- mean((y.test - alpha0.predicted)^2)
MSE0

######### MAE
numeric.test.y <- as.numeric(test.lable)
MAE0 <- mean(abs(numeric.test.y - alpha0.predicted))
MAE0

##################################### Create confusion matrix

library(ggplot2)
library(reshape)

source("ConfusionMatrix_Count.R")
source("ConfusionMatrix_Normalize.R")
source("ConfusionMatrix_Plot.R")
source("ConfusionMatrix_GOSE.R")

actual.egos <- as.integer(y.test)
predicted.egos <- as.integer(round(alpha0.predicted))

confusion.matrix <- ConfusionMatrix_GOSE(actual.egos, predicted.egos)
confusion.matrix$Graph

######################################### Testing on Leuven data
Leuvendata <- read.csv("Data/Leuvendata.CSV")

Leuven.test.features <- Leuvendata[, c("motor", "pupil", "age", "CT")]
Leuven.x.test <- as.matrix(Leuven.test.features)

Leuven.test.lable <- Leuvendata[, c("eGOS")]
Leuven.y.test <- as.matrix(Leuven.test.lable)


Leuven.alpha0.predicted <-
  predict(alpha0.fit, s=alpha0.fit$lambda.1se, newx=Leuven.x.test)


Leuven.MSE <-mean((Leuven.y.test - Leuven.alpha0.predicted)^2)
Leuven.MAE <- mean(abs(Leuven.y.test - Leuven.alpha0.predicted))
Leuven.MSE
Leuven.MAE


########################o Confusion matrix
library(ggplot2)
library(reshape)

source("ConfusionMatrix_Count.R")
source("ConfusionMatrix_Normalize.R")
source("ConfusionMatrix_Plot.R")
source("ConfusionMatrix_GOSE.R")

Leuven.actual.egos <- as.integer(Leuven.y.test)
Leuven.predicted.egos <- as.integer(round(Leuven.alpha0.predicted))

Leuven.confusion.matrix <- ConfusionMatrix_GOSE(Leuven.actual.egos, Leuven.predicted.egos)
Leuven.confusion.matrix$Graph


######################################### Testing on ProTECT data
ProTECTdata <- read.csv("Data/ProTECTdata.CSV")

ProTECT.test.features <- ProTECTdata[, c("motor", "pupil", "age", "CT")]
ProTECT.x.test <- as.matrix(ProTECT.test.features)

ProTECT.test.lable <- ProTECTdata[, c("eGOS")]
ProTECT.y.test <- as.matrix(ProTECT.test.lable)


ProTECT.alpha0.predicted <- predict(alpha0.fit, s=alpha0.fit$lambda.1se, newx=ProTECT.x.test)

ProTECT.MSE <-mean((ProTECT.y.test - ProTECT.alpha0.predicted)^2)
ProTECT.MAE <- mean(abs(ProTECT.y.test - ProTECT.alpha0.predicted))
ProTECT.MSE
ProTECT.MAE

######################## Confusion matrix
library(ggplot2)
library(reshape)

source("ConfusionMatrix_Count.R")
source("ConfusionMatrix_Normalize.R")
source("ConfusionMatrix_Plot.R")
source("ConfusionMatrix_GOSE.R")

ProTECT.actual.egos <- as.integer(ProTECT.y.test)
ProTECT.predicted.egos <- as.integer(round(ProTECT.alpha0.predicted))

ProTECT.confusion.matrix <- ConfusionMatrix_GOSE(ProTECT.actual.egos, ProTECT.predicted.egos)
ProTECT.confusion.matrix$Graph
